{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a47a625b-83ba-4f15-a7b2-b1da5978bdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "huggingfacehub_api_token = 'add_your_api_token'\n",
    "\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8494a4-ec97-4456-b112-6d62614f6f1e",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c9e89eb-699e-4298-8d21-554b75e7fef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138/2837855461.py:3: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../datasets/B2W-Reviews01.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/B2W-Reviews01.csv')\n",
    "df = df[['product_name']]\n",
    "df = df.drop_duplicates(subset = \"product_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5992b-a13d-40cb-af5f-ffdc521ffbf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Function to run the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c3210b7-0261-4470-9a6f-3a32d4b3f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunLLM(llm, df, question_text, template):\n",
    "    inicio = time.time()\n",
    "    result_list = []\n",
    "    count = 0\n",
    "    \n",
    "    for text in df['product_name']:\n",
    "        try: \n",
    "            question = f\"{question_text} {text}\"\n",
    "            prompt = PromptTemplate.from_template(template)\n",
    "            llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "            print(f'{count} - Text: {text}')\n",
    "            result = llm_chain.run(question)\n",
    "            print(f'{result}\\n')\n",
    "            result_list.append(result.split('\\n')[0])\n",
    "            count = count + 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            result_list.append('error')\n",
    "            count = count + 1\n",
    "\n",
    "    fim = time.time()\n",
    "\n",
    "    tempo_decorrido = fim - inicio\n",
    "    print(\"Tempo decorrido:\", tempo_decorrido, \"segundos\\n\")\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af57f182-0797-4011-9fb8-e90253a51d95",
   "metadata": {},
   "source": [
    "# Running algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6d5c752-c5a0-42a5-9d05-4ae1e54a8894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voicelab/vlt5-base-keywords\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      "Asus Vivobook Max, Intel Celeron Quad Core 4GB 500GB\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      "Copo Acrílico, Copo Acrílico, Copo Acríli\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "Philips, Panela de Pressão Elétrica, Panela de Pressão El\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      "Betoneira Columbus, Roma Brinquedos, Roma Brinquedos, Roma Br\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "LG 43UJ6525, LG 43UJ6525, LG 43\n",
      "\n",
      "Tempo decorrido: 0.7163448333740234 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "google/flan-t5-xxl\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      "Branco, Celeron, Core, Intel, LED, Notebook, Tela, Windows\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      "acilico, canudo, cie, copo\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "presso, panela, philips, walita\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      "brinquedos, columbus, roma\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "connection, hdmi, ips, tv\n",
      "\n",
      "Tempo decorrido: 0.7094941139221191 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "bigscience/bloom\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      ". The Asus Vivobook Max X541NA is a good choice for those who don't have a large budget to work with. The Asus Vivobook Max X541NA is a good choice for those who don't have a large budget to work with. This laptop has a 15.6 inch screen with a 1440 x 900 resolution. This laptop has a 15.6 inch screen with a 1440 x 900 resolution. It's powered by a 1.4GHz Intel Celeron N3350 processor and it comes with 4GB of RAM. It's powered by a 1.4GHz Intel Celeron N3350 processor and it comes with 4GB of RAM. The Asus Vivobook Max X541NA has an Intel HD graphics card.\n",
      "The Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco is the perfect laptop for people who do not have much money to spend. This laptop has a 15.6 inch screen with a 1440 x 900 resolution. This laptop has a 15.6 inch screen with a 1440 x 900 resolution. It's powered by a 1.4GHz Intel Celeron N3350 processor and it comes with 4GB of RAM. It's powered by a 1.4GHz Intel Celeron N3350 processor and it comes with 4GB of RAM. The Asus Vivobook Max X541NA has an Intel HD graphics card. The Asus Vivobook Max X541NA has an Intel HD graphics card.\n",
      "The Asus Vivobook Max X541NA is a good choice for those who don't have a large budget to work with. The Asus Vivobook Max X541NA is a good choice for those who don't have a large budget to work with. The Asus Vivobook Max X541NA is a good choice for those who don't have a large budget to work with. This laptop has a 15.6 inch screen with a 1440 x 900 resolution. The Asus Vivobook Max X541NA is a good choice for those who don't have a large budget to work with. The Asus Vivobook Max X541NA is a good choice for those who don't have a large budget to work with.\n",
      "With a 15.6 inches size, you're going to have some trouble carrying it around. The 1.4GHz Intel Celeron N3350 processor and 4GB of RAM won't be able to handle everything you throw\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      " N1056 | Copo com Canudo | Copo com Canudo 500ml | Copo com Canudo Acrílico | Copo com Canudo Acrílico 500ml | Copo com Canudo Acrílico Rocie N1056\n",
      "Of course, you can find a great variety of basic and inexpensive items that you may not use. The problem is that when you need them, you may not have them. That is why we have collected some tips for organizing your kitchen.\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "\n",
      "This is the final piece of the puzzle. The last step of this process is to return a list of the most relevant words in the given context. For this, the corpus is used. It contains the most common words of the language and allows to quickly filter them. This step is very important, as it allows to eliminate common words that have a high frequency, but are not very relevant to the context.\n",
      "The last step of the algorithm returns a list of words that are not common, and are therefore very relevant to the context. This is the list of words to be searched in the dictionary. If one of these words is not found in the dictionary, the algorithm can not return a translation. The translation is obtained by the word that appears in the dictionary with the highest score. The score of each word is calculated by the following formula:\n",
      "The result of this calculation is a number between 0 and 1. The higher the result, the higher the score of the word. This allows to establish a ranking of the words in the dictionary.\n",
      "This process is repeated for each word of the text to be translated. This allows the algorithm to obtain a list of possible translations, and to choose the most appropriate.\n",
      "The algorithm was implemented in Python. It was possible to use the NLTK library, which allows to work with natural language text. In addition, the algorithm uses a large dictionary of Portuguese words. The dictionary was created by scraping Wikipedia.\n",
      "As already mentioned, the dictionary is created by scraping Wikipedia. This allows to obtain a large number of words, with high quality. It is important to note that some of the words are not valid in the language. For example, words with accented letters. This problem can be solved by removing them from the dictionary. It is possible to solve this problem by making the dictionary more generic.\n",
      "In the case of the Portuguese language, this is not a problem. In the case of a language like Japanese, it is more complicated. The Japanese language has a large number of characters. It is not possible to obtain a large dictionary by scraping Wikipedia. This problem is solved by using the Google Translate API.\n",
      "The Google Translate API allows to obtain a list of words that are in the dictionary. In addition, it allows to know the number of times that a word appears. This allows to calculate the frequency of a word. This is very useful to calculate the score of a word, as shown in the previous section.\n",
      "The Google Translate API does not allow to obtain the whole dictionary. This is due\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      ", Loja de Brinquedos. Find all words starting with a letter or word: Your results will be displayed within seconds. Have fun. 1) In order to use this website we recommend that you enable JavaScript. Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for. Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for. Home.\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      ".\n",
      "Some people, when confronted with the reality of a situation, respond with a calm and reasoned analysis of the facts, using logic to arrive at a rational solution. Other people, in the same situation, become enraged, lash out in anger, and resort to violence.\n",
      "The latter is the path of the beast, the former that of the man. Both paths are logical, but one is the path of rationality and the other is that of animal emotion. The problem with the path of animal emotion is that, while it may provide an emotional catharsis, it does nothing to change the situation.\n",
      "The path of the beast is one of self-deception, of trying to change the facts by changing the way you see them. It’s a path of wanting to gain control over reality, rather than letting reality take control of you. It’s a path of “If I could just get away from this reality, then I could make it better.” It’s a path of “I know I’m going to be punished for what I’ve done, so why shouldn’t I punish them too?” It’s a path of “There’s no point in accepting reality, so why try?”\n",
      "You see, when a person is enraged, they are no longer in control of their emotions. They are no longer able to rationally analyze the facts and logically determine the best course of action. They are no longer able to use their intellect to determine what is and isn’t moral. Instead, they are at the mercy of their emotions. They have allowed their emotions to dictate how they respond to a situation. And, because emotions are irrational, they have no control over what they do when their emotions are in control.\n",
      "There is no right or wrong way to respond to the world. There are only different paths. The path of the man and the path of the beast are two such paths.\n",
      "So, if you find yourself enraged, angry, and violent, stop and ask yourself, “Why am I reacting this way? What is causing this anger?” The answer may be a revelation, and you may find that you are able to respond with calm and reason. Or it may be that you are a lost cause. Either way, you deserve to know.\n",
      "i really like how you bring up this topic. this is a very important topic that everyone should think about because it is very important. i really like how you bring up the two different paths and how you explain each one and the effects of each one. this is\n",
      "\n",
      "Tempo decorrido: 0.7174742221832275 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "facebook/blenderbot-400M-distill\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      " Wow, that's a lot of branches. Do you have a favorite branco?\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      " I have never heard of that one. What is it about? Is it a book?\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      " I'm not sure what that is, but it sounds interesting. I'll have to look it up.\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      " Beta Colombus is an American singer, songwriter, dancer, and actress.\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      " Wow, that's a lot of information. I'll have to check that out. Thanks!\n",
      "\n",
      "Tempo decorrido: 0.7833316326141357 segundos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_text = f\"Based in the following phrase, return a list of the most relevant words:\"        \n",
    "template = \"\"\"{question}\"\"\"\n",
    "\n",
    "repo_id = 'Voicelab/vlt5-base-keywords'\n",
    "print(f'{repo_id}')\n",
    "llm_vlt5 = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_vlt5,df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'google/flan-t5-xxl'\n",
    "print(f'{repo_id}')\n",
    "llm_google = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_google, df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'bigscience/bloom'\n",
    "print(f'{repo_id}')\n",
    "llm_bigscience = HuggingFaceEndpoint(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_bigscience, df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'facebook/blenderbot-400M-distill'\n",
    "print(f'{repo_id}')\n",
    "llm_face = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_face, df, question_text, template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0b18166-6c32-4f3a-a107-d57832045fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voicelab/vlt5-base-keywords\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      "Asus Vivobook Max, Asus Vivobook Max, Windows 10, Windows 10\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      "Copo Acrílico, Copo Acrílico, Copo Acríli\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "Philips Walita Daily, Panela de Pressão Elétrica, Pan\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      "Betoneira Columbus, Roma Brinquedos, betoneira, betoneira \n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "LG 43UJ6525, LG 43UJ6525, LG 43\n",
      "\n",
      "Tempo decorrido: 2.9820737838745117 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "google/flan-t5-xxl\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      "asus, core, gb, tela, vivobook, window\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      "acilico, canudo, com, rocie\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "presso, panela, philips, timer\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      "brinquedos, columbus, roma\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "connection, hdmi, ips, tv\n",
      "\n",
      "Tempo decorrido: 4.178520202636719 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "bigscience/bloom\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      " The top result for this query is ASUS MAX X541NA-GO472T. The answer contains the words: Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      " Copo Acrílico Com Canudo 500ml Rocie, [...]\n",
      "As expected, the list above contains the words \"copo\", \"acrílico\" and \"canudo\". We can see that the key \"copo\" (which is the base word) is repeated twice, once with a capital letter and once with a lowercase letter, as expected.\n",
      "However, if we ask for a word that is not in the base list, we should not get any results. For instance, if we ask for \"aguarde\" (\"wait\"), we should not get any results, since this word does not appear in the base list:\n",
      "Question: Based in the following phrase, return a list of the most relevant words: Aguarde\n",
      "Answer: Aguarde\n",
      "\n",
      "My code, based on the code of the tutorial:\n",
      "import nltk\n",
      "import re\n",
      "\n",
      "# nltk.download('punkt')\n",
      "# nltk.download('stopwords')\n",
      "\n",
      "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
      "punkt = nltk.data.load('punkt.pickle')\n",
      "\n",
      "def extract_words(sentence):\n",
      "    words = nltk.word_tokenize(sentence)\n",
      "    return [word for word in words if word not in stopwords]\n",
      "\n",
      "def create_words_dict():\n",
      "    words_dict = {}\n",
      "    for line in open('./data/words.txt'):\n",
      "        words_dict[line.split('\\t')[0].strip()] = line.split('\\t')[1].strip()\n",
      "\n",
      "    return words_dict\n",
      "\n",
      "words_dict = create_words_dict()\n",
      "\n",
      "def create_sentence_dict(sentence):\n",
      "    words = extract_words(sentence)\n",
      "    sentence_dict = {}\n",
      "    for word in words:\n",
      "        if word in words_dict:\n",
      "            sentence_dict[word] = 1\n",
      "    return sentence_dict\n",
      "\n",
      "sentences_dict = {}\n",
      "\n",
      "def get_sentences(text):\n",
      "    sentences = re.split('[.!?]', text)\n",
      "    sentences_dict[sentences[0]] = sentences[1:]\n",
      "    return sentences_dict\n",
      "\n",
      "text = 'Copo Acrílico Com Canudo 500ml Rocie. Aguarde.'\n",
      "sentences = get_sentences(text)\n",
      "for sentence in sentences:\n",
      "    words = extract_words(sentence)\n",
      "    sentence_dict = create_sentence_dict(sentence)\n",
      "    for word in words:\n",
      "        print('Question: Based in the following phrase, return a list of the most\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "\n",
      "The most relevant words are: panela, pressão, elétrica, philips, walita, daily, timer.\n",
      "This was the final answer I got:\n",
      "http://php.net/manual/pt_BR/function.preg-match-all.php\n",
      "<?php\n",
      "function more_relevant_words($phrase)\n",
      "{\n",
      "    $words = preg_split('/\\s+/', $phrase);\n",
      "    array_splice($words, count($words) - 3, 3, array(\"e\", \"e\"));\n",
      "    $words = array_reverse($words);\n",
      "    $words = array_slice($words, 0, 3);\n",
      "    return $words;\n",
      "}\n",
      "\n",
      "echo more_relevant_words(\"Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\");\n",
      "\n",
      "A:\n",
      "\n",
      "You can try using explode() function of php. You can try with the below code.\n",
      "<?php\n",
      "\n",
      "$string = \"Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\";\n",
      "\n",
      "$parts = explode(\" \",$string);\n",
      "\n",
      "for($i = 0; $i < count($parts); $i++){\n",
      " echo $parts[$i];\n",
      "}\n",
      "\n",
      "?>\n",
      "\n",
      "A:\n",
      "\n",
      "You can use explode function to separate the words in the phrase and then use array_count_values to count how many times each word appears in the phrase. Then use array_keys to get the most used words. And finally use implode function to combine the most used words in the phrase. Try the following code:\n",
      "<?php\n",
      "\n",
      "function more_relevant_words($phrase)\n",
      "{\n",
      "    $words = explode(\" \", $phrase);\n",
      "\n",
      "    $count = array_count_values($words);\n",
      "\n",
      "    $top3 = array_keys($count, max($count));\n",
      "\n",
      "    return implode(\" \", $top3);\n",
      "}\n",
      "\n",
      "echo more_relevant_words(\"Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\");\n",
      "\n",
      "Hope this helps you.\n",
      "Update\n",
      "Here is the updated code with dynamic words counting:\n",
      "<?php\n",
      "\n",
      "function more_relevant_words($phrase, $wordsCount)\n",
      "{\n",
      "    $words = explode(\" \", $phrase);\n",
      "\n",
      "    $count = array_count_values($words);\n",
      "\n",
      "    $top3 = array_keys($count, max($count));\n",
      "\n",
      "    return implode(\" \", array_slice($top3, 0, $wordsCount));\n",
      "}\n",
      "\n",
      "echo more_relevant_\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      " The list should contain the most relevant words, that is, the words that appear most often in the document (usually more than 3 times).\n",
      "Lets say I have a document that contains the following words (as a String):\n",
      "Betoneira Columbus - Roma Brinquedos\n",
      "\n",
      "Now I would like to return a list containing the following words: Betoneira, Columbus, Roma, Brinquedos.\n",
      "The list should be sorted by frequency (number of times the word appears in the document), so that it would return the following words:\n",
      "Betoneira\n",
      "Columbus\n",
      "Roma\n",
      "Brinquedos\n",
      "\n",
      "A:\n",
      "\n",
      "If you want to return only words that appear more than once, use this code to count them:\n",
      "public static void main(String[] args) throws FileNotFoundException {\n",
      "    List<String> list = new ArrayList<>();\n",
      "    list.add(\"Betoneira Columbus - Roma Brinquedos\");\n",
      "    list.add(\"Betoneira Columbus - Roma Brinquedos\");\n",
      "    list.add(\"Betoneira Columbus - Roma Brinquedos\");\n",
      "\n",
      "    String s = \"Betoneira Columbus - Roma Brinquedos\";\n",
      "    Set<String> set = new HashSet<>();\n",
      "    for (int i = 0; i < list.size(); i++) {\n",
      "        set.addAll(Arrays.asList(list.get(i).split(\" \")));\n",
      "    }\n",
      "    List<String> mostCommon = new ArrayList<>();\n",
      "    for (String str : set) {\n",
      "        if (set.size() - set.count(str) >= 2) {\n",
      "            mostCommon.add(str);\n",
      "        }\n",
      "    }\n",
      "    mostCommon.stream().sorted().forEachOrdered(System.out::println);\n",
      "}\n",
      "\n",
      "Output:\n",
      "Betoneira\n",
      "Columbus\n",
      "Roma\n",
      "Brinquedos\n",
      "\n",
      "If you want to return only words that appear more than 3 times, replace set.size() - set.count(str) >= 2 with set.size() - set.count(str) >= 3.\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      " [\"Smart\", \"TV\", \"LED\", \"LG\", \"43UJ6525\", \"Ultra\", \"HD\", \"4K\", \"com\", \"Conversor\", \"Digital\", \"4\", \"HDMI\", \"2\", \"USB\", \"WebOS\", \"3.5\", \"Painel\", \"Ips\", \"HDR\", \"e\"]\n",
      "A baseline method for this problem is to find the top-k most frequent words in the given phrases and return them. This method works well for short phrases but fails for longer phrases where most of the words are rare words. Here, a baseline method uses a bag-of-word approach and calculates the frequency of each word in the given phrase. The top-k most frequent words in the given phrases are returned. Here, k is a parameter which decides the number of words to be returned.\n",
      "This baseline method is ineffective for the given problem because it fails to consider the context of the words. For example, consider the phrase “car”, a baseline method might return “manufacture”, “factory”, “machine” as the most frequent words. But, we know that the word “car” is more relevant than those words. A similar situation is with the phrase “car”, a baseline method might return “manufacture”, “factory”, “machine” as the most frequent words. But, we know that the word “car” is more relevant than those words. A similar situation is with the phrase “sport car”. The word “car” is more relevant than the word “sport”. In the phrase “smart TV”, the word “TV” is more relevant than the word “smart”. We need a way to model this context information. There are several ways to do this. One way is to use a model similar to a recurrent neural network (RNN). But, such models are difficult to train and requires large datasets. Another way is to use a model similar to the skip-gram model. However, such models are also difficult to train. A third option is to use a neural language model which is pre-trained on a large dataset. This can then be fine-tuned using the small dataset at hand.\n",
      "In this project, we will use a neural language model which is pre-trained on a large dataset. We will then fine-tune this language model using the small dataset at hand. We will use the fine-tuned model to predict the\n",
      "\n",
      "Tempo decorrido: 99.12536764144897 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "facebook/blenderbot-400M-distill\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      " Hello, how are you today? I just got back from a walk with my dog. \n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      " Hello, how are you doing today? I am well, thank you for asking. \n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      " Hi, how are you today? I just got back from a walk in the park.\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      " I'm not sure what you mean by an \"Anne of Green Gables\"\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      " I'm not sure what you mean by that, but I do know that the 43-43 LED TV is the most widely used television system in the world.\n",
      "\n",
      "Tempo decorrido: 15.500394821166992 segundos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_text = f\"Based in the following phrase, return a list of the most relevant words:\"        \n",
    "template = \"\"\"Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "repo_id = 'Voicelab/vlt5-base-keywords'\n",
    "print(f'{repo_id}')\n",
    "llm_vlt5 = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_vlt5,df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'google/flan-t5-xxl'\n",
    "print(f'{repo_id}')\n",
    "llm_google = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_google, df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'bigscience/bloom'\n",
    "print(f'{repo_id}')\n",
    "llm_bigscience = HuggingFaceEndpoint(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_bigscience, df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'facebook/blenderbot-400M-distill'\n",
    "print(f'{repo_id}')\n",
    "llm_face = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_face, df, question_text, template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82208bf6-087b-4728-a67c-0309aaf74669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voicelab/vlt5-base-keywords\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      "Asus Vivobook Max, Windows 10 Branco, Windows 10 Branco, Windows 10\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      "Copo Acrílico, Copo Acrílico, Copo Acríli\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "Panela de Pressão Elétrica Philips Walita Daily, Panela\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      "Betoneira Columbus, Roma Brinquedos, Roma Brinquedos, Roma Br\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "LG 43UJ6525, LG 43UJ6525, LG 43\n",
      "\n",
      "Tempo decorrido: 16.561146020889282 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "google/flan-t5-xxl\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      "asus, core, gb, tela, vivobook, window\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      "acilico, canudo, com, ricie\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      "presso, panela, philips, timer\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      "brinquedos, columbus, roma\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "connection, hdmi, ips, tv\n",
      "\n",
      "Tempo decorrido: 4.1013548374176025 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "bigscience/bloom\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      " ordered by the amount of words which are relevant to the phrase.\n",
      "\n",
      "A:\n",
      "\n",
      "I think that you're not quite understanding the way that wordnet works. You can think of it like a dictionary, where each word has a list of senses (which are word meanings) and a list of synonyms for each sense. So the question you're asking is, \"how do I find synonyms for 'notebook'?\"\n",
      "The easiest way to do that is to go to http://wordnetweb.princeton.edu/perl/webwn, type notebook in the search box, and select the first result. You'll see that it's listed as having two senses, one of which is \"book for jotting down notes and memoranda\" and the other is \"a portable personal computer designed for taking notes and storing them\". The first sense is the one we're interested in. Go to the wordnet page for notebook (http://wordnetweb.princeton.edu/perl/webwn/notebook) and you'll see that the synsets for that sense are:\n",
      "\n",
      "1 n a portable writing or drawing tablet\n",
      "1 n a portable personal computer designed for taking notes and storing them\n",
      "\n",
      "You can also find the synonyms by typing synonyms for notebook into the search box. The first result is that the synsets are 1 n portable writing or drawing tablet, 1 n portable personal computer designed for taking notes and storing them, 2 n portable writing or drawing tablet, 2 n portable personal computer designed for taking notes and storing them. So, you can see that the synonyms for notebook are portable writing tablet and portable personal computer.\n",
      "Hope this helps.\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      " ordered by length, meaning longer words are more relevant than shorter ones. In case of a tie, the order is determined by the first appearance in the text. This means that the word Rocie is more relevant than Acrílico because it first appears in the text. The word Acrílico is more relevant than Com because it first appears in the text.\n",
      "Answer: Not necessarily, but if the word Rocie is not present in the text, Acrílico is not relevant, either. If the word Com is not present in the text, neither Acrílico nor Rocie are relevant, because both are only relevant if they appear in the text before Com.\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      " [\"cozinhar\", \"pressão\", \"panela\", \"timer\"]\n",
      "I've tried to implement this through the following code, but it's not working as expected. Any suggestions to improve this approach would be very appreciated.\n",
      "# the function receives a phrase and a dictionary, the dictionary is \n",
      "# from a file which contains a list of words and their frequency of occurrence.\n",
      "def most_relevant_words(phrase, dictionary):\n",
      "    # compute the tfidf of each word in the phrase\n",
      "    t = {}\n",
      "    for word in phrase.split():\n",
      "        t[word] = dictionary[word]\n",
      "    # find the most relevant words\n",
      "    most_relevant_words = []\n",
      "    for key in dictionary:\n",
      "        if t[key] > 0:\n",
      "            most_relevant_words.append(key)\n",
      "    return most_relevant_words\n",
      "\n",
      "A:\n",
      "\n",
      "It looks like you are trying to apply the TF-IDF method to a document (the phrase) and a corpus (the dictionary). This will give you a list of words that are important to your document, in comparison to the corpus.\n",
      "A TF-IDF (term frequency - inverse document frequency) score gives you a measure of how important a word is in a document, compared to the rest of the corpus. The score is the number of times a word occurs in a document, divided by the total number of words in the document, multiplied by the log of the total number of documents divided by the number of documents the word appears in.\n",
      "I have used the following dictionary (from the SentiWordNet project) as my corpus. The words are separated into categories, so it is easy to find the most relevant words for a particular category. You will need to replace this with your own dictionary.\n",
      "categories = {\n",
      "    'Animals': {'horse', 'cat', 'elephant', 'mouse'},\n",
      "    'Food': {'bread', 'apple', 'fish', 'meat'},\n",
      "    'Misc': {'man', 'car', 'house', 'window'}\n",
      "}\n",
      "\n",
      "Using the above dictionary, the following code will return a list of words from a category, in descending order of TF-IDF score.\n",
      "from collections import Counter\n",
      "\n",
      "def tf_idf(phrase, corpus, category):\n",
      "    # convert to lower case\n",
      "    phrase = phrase.lower()\n",
      "    # get the set of words in the document\n",
      "    words = set(phrase.split())\n",
      "    # get the set of words in the category\n",
      "    category_words =\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      " as follows: Betoneira, Columbus, Roma, Brinquedos\n",
      "My code:\n",
      "from nltk.corpus import wordnet as wn\n",
      "\n",
      "def most_relevant_words(phrase):\n",
      "    synsets = wn.synsets(phrase)\n",
      "    synsets.remove(synsets[0])\n",
      "    # remove stopwords\n",
      "    return [synset.name.lower() for synset in synsets if not synset.name.lower() in wn.stopwords('portuguese')]\n",
      "\n",
      "The code above works but returns a list of words in alphabetical order. For example, if I provide the phrase: Betoneira Columbus - Roma Brinquedos, the output is: ['columbus', 'betoneira', 'roma', 'brinquedos']\n",
      "I would like to have the words in the following order: ['betoneira', 'columbus', 'roma', 'brinquedos']\n",
      "What do I need to change in my code?\n",
      "\n",
      "A:\n",
      "\n",
      "Sort your list and remove the first element:\n",
      "return sorted(synsets, key=lambda syn: syn.name.lower(), reverse=True)[1:]\n",
      "\n",
      "This uses the lambda function to sort the synsets based on the name. Then the reverse=True parameter reverses the order. Finally, we remove the first element of the list with the[1:].\n",
      "\n",
      "A:\n",
      "\n",
      "To be honest, I am not sure what you are looking for. If you want the most relevant words, I would try to simply select the synset with the highest rank:\n",
      "def most_relevant_words(phrase):\n",
      "    synsets = wn.synsets(phrase)\n",
      "    synsets.remove(synsets[0])\n",
      "    # remove stopwords\n",
      "    return [synset.name.lower() for synset in synsets]\n",
      "\n",
      "I believe that if you want to get a ranking of the words, you have to add more information to the synset, e.g. its frequency in a corpus.\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      " the following: 43, lg, 43uj6525, ultra, led, smart, tv, hdmi, webos, magic, mobile, connection, hdr, ips, digital, conversor, 4k, lg\n",
      "Question: The following phrase is the basis of the exercise: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "Answer: The list is the following: 43, lg, 43uj6525, ultra, led, smart, tv, hdmi, webos, magic, mobile, connection, hdr, ips, digital, conversor, 4k, lg\n",
      "Answer: The answer is the following: lg, 43uj6525, ultra, led, smart, tv, hdmi, webos, magic, mobile, connection, hdr, ips, digital, conversor, 4k, lg\n",
      "Question: The following phrase is the basis of the exercise: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "Answer: The list is the following: 43, lg, 43uj6525, ultra, led, smart, tv, hdmi, webos, magic, mobile, connection, hdr, ips, digital, conversor, 4k, lg\n",
      "Question: Based in the following phrase, return a list of the most relevant words: TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "Answer: The list is the following: 43, lg, 43uj6525, ultra, led, smart, tv, hdmi, webos, magic, mobile, connection, hdr, ips, digital, conversor, 4k, lg\n",
      "Question: The following phrase is the basis of the exercise: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      "Answer: The list is the following: 43, lg, 43uj6525, ultra, led, smart, tv, hdmi, webos, magic, mobile, connection, hdr, ips, digital, conversor, 4k, lg\n",
      "Question:\n",
      "\n",
      "Tempo decorrido: 93.3576328754425 segundos\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "facebook/blenderbot-400M-distill\n",
      "0 - Text: Notebook Asus Vivobook Max X541NA-GO472T Intel Celeron Quad Core 4GB 500GB Tela LED 15,6\" Windows - 10 Branco\n",
      " I'm not sure what you are trying to say.  What is the list?\n",
      "\n",
      "1 - Text: Copo Acrílico Com Canudo 500ml Rocie\n",
      " I'm not sure what you are trying to say, but I can tell you that \"Copo\" means \"little armoured one\" in Spanish.\n",
      "\n",
      "2 - Text: Panela de Pressão Elétrica Philips Walita Daily 5L com Timer\n",
      " The Question is one of my favorite things to do on the weekends. What do you like to do in your spare time?\n",
      "\n",
      "3 - Text: Betoneira Columbus - Roma Brinquedos\n",
      " I'm not sure what you are trying to say, but I can tell you that Beta Colombus is an American singer, songwriter, dancer, and actress.\n",
      "\n",
      "4 - Text: Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com Conversor Digital 4 HDMI 2 USB WebOS 3.5 Painel Ips HDR e Magic Mobile Connection\n",
      " I'm not sure what you are trying to say.  What is the list?\n",
      "\n",
      "Tempo decorrido: 19.410956621170044 segundos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_text = f\"Based in the following phrase, return a list of the most relevant words:\"        \n",
    "template = \"\"\"Question: {question}\n",
    "Answer: The list is\"\"\"\n",
    "\n",
    "repo_id = 'Voicelab/vlt5-base-keywords'\n",
    "print(f'{repo_id}')\n",
    "llm_vlt5 = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_vlt5,df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'google/flan-t5-xxl'\n",
    "print(f'{repo_id}')\n",
    "llm_google = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_google, df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'bigscience/bloom'\n",
    "print(f'{repo_id}')\n",
    "llm_bigscience = HuggingFaceEndpoint(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_bigscience, df, question_text, template))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "\n",
    "repo_id = 'facebook/blenderbot-400M-distill'\n",
    "print(f'{repo_id}')\n",
    "llm_face = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "all_results.append(RunLLM(llm_face, df, question_text, template))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
